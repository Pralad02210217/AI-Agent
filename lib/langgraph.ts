import { ChatAnthropic } from "@langchain/anthropic"
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HarmBlockThreshold, HarmCategory } from "@google/generative-ai";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import wxflows from "@wxflows/sdk/langchain";
// Add at the top
import { GeminiToolCall } from "@/lib/types";
import {
    END,
    MemorySaver,
    MessagesAnnotation,
    START,
    StateGraph,
  } from "@langchain/langgraph";
import SYSTEM_MESSAGE from "@/constants/systemMessage";
import {
    ChatPromptTemplate,
    MessagesPlaceholder,
  } from "@langchain/core/prompts";
import { AIMessage, BaseMessage, HumanMessage, SystemMessage, trimMessages } from "@langchain/core/messages";


//Customers at: https://introspection.apis.stepzen.com/customers
// Commetents at: https://dummyjson.com/comments

// Trim the messages to manage converstaiotn history
// const trimmer = trimMessages({
//     maxTokens: 10,
//     strategy: "last",
//     tokenCounter: (msgs) => msgs.length,
//     includeSystem: true,
//     allowPartial: false,
//     startOn: "human"
// })
const trimmer = trimMessages({
  maxTokens: 4000,
  strategy: "last",
  tokenCounter: async (msgs) => {
    const total = msgs.reduce((acc, m) => acc + Math.ceil(m.content.length / 4), 0);
    return total;
  },
  includeSystem: true,
  allowPartial: false,
  startOn: "human"
});

// Connect to wxflows
const toolClient = new wxflows({
    endpoint: process.env.WXFLOWS_ENDPOINT || "",
    apikey: process.env.WXFLOWS_APIKEY,
  });
  
  // Retrieve the tools
  const tools = await toolClient.lcTools;
  const toolNode = new ToolNode(tools)

  
const initialiseModel = () => {
    return new ChatGoogleGenerativeAI({
      modelName: "gemini-1.5-flash",
      apiKey: process.env.GEMINI_API_KEY,
      temperature: 0.7,
      maxOutputTokens: 4000,
       
      safetySettings: [
        { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_NONE },
        { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_NONE },
        { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_NONE },
        { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_NONE }
      ],
    }).bindTools(tools);
  };

function shouldContinue(state: typeof MessagesAnnotation.State) {
    const lastMessage = state.messages[state.messages.length - 1] as AIMessage;
    
    // const toolCalls = lastMessage.additional_kwargs?.tool_calls;
    // console.log("Tool calls generated by the model:", toolCalls);

    // if (toolCalls?.length) {
    //   console.log("AI is making tool calls:", toolCalls);
    //   return "tools";
    // }

    // //     // If the LLM makes a tool call, then we route to the "tools" node
    if (lastMessage.tool_calls?.length) {
      return "tools";
    }
  
    
    if (lastMessage.content && lastMessage._getType() === "tool") {
      console.log("AI not using any tools lol")
      return "agent";
    }
    return END;
  }
  

  const createWorkflow = () => {
    const model = initialiseModel();
  
    const stateGraph = new StateGraph(MessagesAnnotation)
      .addNode("agent", async (state) => {
        // Create the system message content
        const systemContent = SYSTEM_MESSAGE;
  
        // Create the prompt template with system message and messages placeholder
        const promptTemplate = ChatPromptTemplate.fromMessages([
          new SystemMessage(systemContent, {
            cache_control: { type: "ephemeral" },
          }),
          new MessagesPlaceholder("messages"),
        ]);
  
        // Trim the messages to manage conversation history
        const trimmedMessages = await trimmer.invoke(state.messages);
  
        // Format the prompt with the current messages
        const prompt = await promptTemplate.invoke({ messages: trimmedMessages });
        console.log(prompt)

        
  
        // Get response from the model
        const response = await model.invoke(prompt);
        console.log(response)
  
        return { messages: [response] };
      })
      .addEdge(START, "agent")
      .addNode("tools", toolNode)
      .addConditionalEdges("agent", shouldContinue)
      .addEdge("tools", "agent");

      return stateGraph
  };




export async function submitQuestion(messages: BaseMessage[], chatId: string) {
    const workflow = createWorkflow();
    const checkpointer = new MemorySaver();
    const app = workflow.compile({ checkpointer });
  
    const stream = await app.streamEvents(
      { messages },
      {
        version: "v2",
        configurable: { thread_id: chatId },
        streamMode: 'messages',
        runId: chatId
      }
    );
  
    return stream;
  }